{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing - continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "custom_palette = sns.color_palette('viridis', 2)\n",
    "sns.set_palette(custom_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our sample of movies to perform hypothesis testing in two ways: model of the world and confidence interval. \n",
    "\n",
    "Say that someone claims that the IMDB score given to each movie is a completely random number drawn between 1 (minimum) and 10 (maximum). This then implies that the average score should be 5.5.\n",
    "\n",
    "$H_{0}$: Since IMDB scores of movies are random, the mean score in the population is 5.5.<br>\n",
    "$H_{1}$: No, scores are not random, and the mean score is different than 5.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('movie_metadata.csv')\n",
    "display(movies_df.describe())\n",
    "sample_size = movies_df.shape[0]\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_imdb_scores():\n",
    "    random_scores = np.random.uniform(1, 10, size=sample_size)\n",
    "    mean_score = random_scores.mean()\n",
    "    return mean_score\n",
    "\n",
    "# run multiple simulations\n",
    "num_sims = 5000\n",
    "many_mean_scores = np.empty(num_sims)\n",
    "for i in range(num_sims):\n",
    "    many_mean_scores[i] = simulate_imdb_scores()\n",
    "    \n",
    "many_mean_scores\n",
    "# Compute p-value\n",
    "observed_stat = movies_df['imdb_score'].mean() #get value of test statistic from data\n",
    "num_samples_with_stat_we_got_or_less = np.count_nonzero(many_mean_scores >= observed_stat)\n",
    "print('The p-value is', num_samples_with_stat_we_got_or_less/num_sims)\n",
    "\n",
    "# visualizing the results\n",
    "ax = sns.displot(many_mean_scores)\n",
    "ax.set(xlabel='Average score in sample of movies', ylabel='Number of simulations')\n",
    "plt.scatter(observed_stat, 0, marker='.', s=200, color='red', clip_on=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, since our alternative hypothesis is **two-sided** (the average is either higher or lower than 5.5), we should have also added all cases in which the simulations showed us more extreme values in the other direction: smaller than the hypothetical value of a test statistic that would be \"as small as our actual test statistic is big\" (relative to $H_{0}$). Hence, here, we would also add up all simulations in which the mean scores were lower than 5.5-(6.46-5.5) = 4.54. Of course, in this case there were no such simulations, so the p-value does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the conclusion?\n",
    "\n",
    "Now, let's create a 95% confidence interval for the mean IMDB score in the population and check the same hypotheses again. What do you expect would happen? Will the CI include 5.5? What if we create a 99% confidence interval? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bootstrap_mean(original_sample, column_name, num_replications):\n",
    "    '''This function returns an array of bootstrapped sample averages:\n",
    "    original_sample: df containing the original sample\n",
    "    column_name: name of column containing the variable of interest\n",
    "    num_replications: number of bootstrap samples'''\n",
    "    original_sample_size = original_sample.shape[0] \n",
    "    original_sample_var_of_interest = original_sample[[column_name]]\n",
    "    bstrap_means = np.empty(num_replications)\n",
    "    for i in range(num_replications):\n",
    "        bootstrap_sample = original_sample_var_of_interest.sample(original_sample_size, replace=True) # note WITH REPLACEMENT!\n",
    "        resampled_mean = bootstrap_sample.mean()\n",
    "        bstrap_means[i] = resampled_mean\n",
    "    \n",
    "    return bstrap_means\n",
    "\n",
    "means_bootstrapped = bootstrap_mean(movies_df, 'imdb_score', 5000)\n",
    "ax=sns.displot(means_bootstrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the endpoints of the 95% confidence interval\n",
    "left_end = np.percentile(means_bootstrapped, 2.5, method='higher')\n",
    "right_end = np.percentile(means_bootstrapped, 97.5, method='higher')\n",
    "print('The 95% boostsrap confidence interval for population mean is', [left_end,right_end])\n",
    "\n",
    "# visualize results\n",
    "ax = sns.displot(means_bootstrapped)\n",
    "plt.hlines(y=0, xmin=left_end, xmax=right_end, colors='orange', linestyles='solid', lw=7, clip_on=False);  # show line of values between 10 and 90 percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of groups\n",
    "\n",
    "So far we only tested hypotheses regarding a single random sample. Often, we would like to compare two random samples to one another. Specifically, we may be interested in knowing if a certain value associated with each of two random samples is the same or are the samples different with respect to that value (this is also sometimes called A/B testing).\n",
    "\n",
    "As an example, let's check whether there is a difference in the gross income of drama and comedy movies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of comedy and drama movies\n",
    "drama_comedy_df = movies_df[(movies_df['genres'] == 'Drama') | (movies_df['genres'] == 'Comedy')]\n",
    "drama_comedy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's visualize the means of these two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=drama_comedy_df, x='genres', y='gross', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is a difference in the averages between the two genres in the sample, but is this difference just due to chance, or is it evidence of a more systamatic difference in the means of the population?\n",
    "\n",
    "First, let's formulate our hypotheses:\n",
    "\n",
    "$H_{0}$: The mean gross income of drama movies is equal to the mean gross income of comedy movies<br>\n",
    "$H_{1}$: The mean gross income of drama movies is **not** equal to the mean gross income of comedy movies\n",
    "\n",
    "(question: why did we formulate the hypotheses this way and not the other way around?)\n",
    "\n",
    "We can use the same process of hypothesis testing through confidence intervals to do this. \n",
    "\n",
    "To test the hypothesis, we can construct a confidence interval for the **difference between the means** of gross income of drama and comedy movies. That is, the parameter we are interested is the **difference between the means** of drama and comedy movies in the entire population. <br>\n",
    "We only have a sample of movies, from which we can construct a confidence interval to estimate the difference in means, with high probability. \n",
    "\n",
    "*Thought exercise:* What values of the confidence interval will cause us to reject the null hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the difference in averages\n",
    "def diff_of_avgs(df, column_name, grouping_var):\n",
    "    grpby_var = df.groupby(grouping_var)\n",
    "    avgs = grpby_var[column_name].mean()\n",
    "#     return avgs.loc['Drama'] - avgs.loc['Comedy']  \n",
    "    return avgs[1] - avgs[0]\n",
    "\n",
    "def bootstrap_mean_difference(original_sample, column_name, grouping_var, num_replications):\n",
    "    '''This function returns an array of bootstrapped differences between two sample averages:\n",
    "      original_sample: df containing the original sample\n",
    "      column_name: name of column containing the variable to average\n",
    "      grouping_var: name of variable according to which to group\n",
    "      num_replications: number of bootstrap samples'''\n",
    "    original_sample_size = original_sample.shape[0] # we need to replicate with the same sample size\n",
    "    original_sample_cols_of_interest = original_sample[[column_name, grouping_var]]\n",
    "    bstrap_mean_diffs = np.empty(num_replications)\n",
    "    for i in range(num_replications):\n",
    "        bootstrap_sample = original_sample_cols_of_interest.sample(original_sample_size, replace=True) # note WITH REPLACEMENT!\n",
    "        resampled_mean_diff = diff_of_avgs(bootstrap_sample, column_name, grouping_var)\n",
    "        bstrap_mean_diffs[i] = resampled_mean_diff\n",
    "    \n",
    "    return bstrap_mean_diffs\n",
    "\n",
    "# run the bootstrap procedure\n",
    "bstrap_diffs = bootstrap_mean_difference(drama_comedy_df, 'gross', 'genres',  5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the endpoints of the 95% confidence interval\n",
    "left_end = np.percentile(bstrap_diffs, 2.5, method='higher')\n",
    "right_end = np.percentile(bstrap_diffs, 97.5, method='higher')\n",
    "print('The 95% boostsrap confidence interval for difference between population means', [left_end,right_end])\n",
    "\n",
    "# visualize results\n",
    "ax = sns.displot(bstrap_diffs)\n",
    "plt.hlines(y=0, xmin=left_end, xmax=right_end, colors='orange', linestyles='solid', lw=7, clip_on=False);  # show line of values between 2.5 and 97.5 percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero is not included in the 95% CI for the difference between the means, and therefore we reject the null hypothesis and conclude at the 95% confidence level that the there is a difference between the mean gross income of comedy movies and drama movies.\n",
    "\n",
    "Note that we actually got a more useful result than just saying \"the difference is not zero\": We have an **estimate of what this difference is**.\n",
    "\n",
    "#### Another example\n",
    "We can now compare other features of the two groups we defined. For example, let's compare imdb_score. \n",
    "First, visualize the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=drama_comedy_df, x='genres', y='imdb_score', kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "Test the hypothesis that the average imdb_score for drama movies is different than the average number of director Facebook likes for comedy movies. **Use 0.01 as significance level.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple categories\n",
    "Previosuly, we tested hypotheses for continuous variables (as above) or for the proportion of a single category (e.g. proportion of black jurors). We can also look at the distribution of multiple categories and test hypotheses for this case. This will require a distance measure that compares probability distributions.\n",
    "\n",
    "### Jury Selection in Alameda County\n",
    "In 2010, the American Civil Liberties Union (ACLU) of Northern California presented a report on jury selection in Alameda County, California. The report concluded that certain ethnic groups are underrepresented among jury panelists in Alameda County, and suggested some reforms of the process by which eligible jurors are assigned to panels. We will perform our own analysis of the data and examine some questions that arise as a result.\n",
    "\n",
    "Let's look at the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury_panel_df = pd.DataFrame({'Ethnicity': ['Asian', 'Black', 'Latino', 'White', 'Other'], \n",
    "                              'Eligible Jury': [0.15, 0.18, 0.12, 0.54, 0.01], \n",
    "                              'Jury Panels': [0.26, 0.08, 0.08, 0.54, 0.04]\n",
    "                             })\n",
    "\n",
    "# reshape df to get the value of interest, proportion, on the same column (tidy up the data)\n",
    "jury_panel_df_reshaped = pd.melt(jury_panel_df, id_vars=\"Ethnicity\", var_name=\"Population\", value_name=\"Proportion\")\n",
    "jury_panel_df_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distributions\n",
    "ax = sns.catplot(kind=\"bar\", x='Ethnicity', y='Proportion', hue='Population', data=jury_panel_df_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess whether the panels are chosen at random (without biasing based on ethnic groups), we will need a statistic that compares *distributions*. \n",
    "\n",
    "There are many distance metrics for distributions, we will use a simple measure, called \"total variation distance\" (https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures). \n",
    "This is how we compute total variation distance:\n",
    "- For each category, compute the difference in proportions between two distributions\n",
    "- Take the absolute value of each difference\n",
    "- Sum, and then divide the sum by 2\n",
    "\n",
    "For example, in the jury case, the distance between the distribution of ethnic groups in the population and in the panels is:\n",
    "$(|0.15-0.26| + |0.18-0.08| + |0.12-0.08| + |0.54-0.54| + |0.01-0.04|)/2 = 0.14$\n",
    "\n",
    "Let's make sure we got this right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_groups = ['Asian', 'Black', 'Latino', 'White', 'Other']\n",
    "population_dist = [0.15, 0.18, 0.12, 0.54, 0.01] # distribution of eligible jurors\n",
    "panels_dist = [0.26, 0.08, 0.08, 0.54, 0.04] # distribution in jury panels\n",
    "\n",
    "# function for computing total variation distance\n",
    "def compute_tvd(sample1_vals, sample2_vals):\n",
    "    return (np.sum(np.absolute(sample1_vals - sample2_vals)))/2\n",
    "\n",
    "tvd_aclu = compute_tvd(np.array(population_dist), np.array(panels_dist))\n",
    "tvd_aclu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simulate random jury panels of size 1453 (sample size of the ACLU data) based on their distribution in the population, and check the total variation distance between the values we get and the distribution in the population. Then, we can see whether a distance of 0.14 is reasonable given our model.\n",
    "\n",
    "First, let's write a code to simulate one value of the statistic, under the null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for simulating one value of the statistic under the null\n",
    "sample_size = 1453\n",
    "def simulate_jury_distance():\n",
    "    # first, draw a simple random sample under the null\n",
    "    jury_sample = np.random.choice(population_groups, p=population_dist, size=sample_size)\n",
    "    # second, count the number of elements of each group\n",
    "    jury_sample_group_counts = []\n",
    "    for group in population_groups:\n",
    "        jury_sample_group_counts.append(np.count_nonzero(jury_sample == group))\n",
    "    # third, get the proportions in the sample\n",
    "    proportions_in_sample = np.divide(jury_sample_group_counts, sample_size)\n",
    "    # finally, get TVD between sample and theoretical distribution\n",
    "    distance = compute_tvd(np.array(population_dist), proportions_in_sample)\n",
    "    return distance\n",
    "\n",
    "simulate_jury_distance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get an empirical distribution of the test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running many simulations\n",
    "num_repetitions = 10000\n",
    "many_tvds = np.empty(num_repetitions)\n",
    "for i in range(num_repetitions):\n",
    "    many_tvds[i] = simulate_jury_distance()\n",
    "    \n",
    "many_tvds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare visually between the distribution of TVDs and the test statistic we got, and compute the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the results\n",
    "ax = sns.displot(many_tvds)\n",
    "ax.set(xlabel='distance between jury distribution and eligible distribution', ylabel='Number of simulations')\n",
    "plt.scatter(tvd_aclu, 0, color='red', s=80, clip_on=False); # shows red dot where the value of the test statistic is\n",
    "\n",
    "num_samples_with_tvd_as_we_got_or_more = np.count_nonzero(many_tvds >= tvd_aclu)\n",
    "print ('The p-value is', num_samples_with_tvd_as_we_got_or_more/num_repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the conclusion?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
